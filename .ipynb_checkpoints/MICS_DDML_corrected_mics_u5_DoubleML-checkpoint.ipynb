{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7a9f3c",
   "metadata": {},
   "source": [
    "# MICS U5 â€” DoubleML (PLR + IRM + APOS) with Hyperparameter Tuning (Optuna)\n",
    "This notebook loads `mics_u5.csv` and runs:\n",
    "- **PLR** (Partial Linear Regression) with multiple treatments\n",
    "- **IRM** (Interactive Regression Model) for binary treatments (one at a time)\n",
    "- **APOS** (Average Potential Outcomes) for discrete multi-level treatments (e.g., `water_treatment3`)\n",
    "- Optional **GATE**/**CATE** post-estimation\n",
    "\n",
    "It is written to be **robust to high-dimensional one-hot covariates** and includes **Optuna-based tuning** using DoubleML's recommended APIs when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8d76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Install compatible versions (recommended) ---\n",
    "# NOTE: DoubleML (as of many stable releases) is not compatible with NumPy>=2.\n",
    "# If you get import errors, run the following cell once, then restart the kernel.\n",
    "# %pip install -U \"numpy<2\" \"scikit-learn>=1.2\" pandas optuna patsy doubleml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DoubleML + ML\n",
    "import doubleml as dml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65221d4",
   "metadata": {},
   "source": [
    "## 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"mics_u5.csv\"  # expects the file in the current working directory OR update path\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"Example columns:\", list(df_raw.columns[:25]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3871bf4",
   "metadata": {},
   "source": [
    "## 2) Choose outcome (Y), treatments (D), and features (X)\n",
    "Defaults below are inferred from the dataset (you can edit them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Suggested defaults based on the uploaded dataset ---\n",
    "y_col = \"diarrhea\"\n",
    "\n",
    "# Binary treatments (IRM applicable)\n",
    "d_cols_binary = [\n",
    "    \"water_treatment\",\n",
    "    \"BetterWaterPostTreatment\",\n",
    "    \"StrictlyBetterWaterPostTreatment\"\n",
    "]\n",
    "\n",
    "# Multi-level discrete treatment (APOS applicable)\n",
    "d_col_multilevel = \"water_treatment3\"   # levels observed: 0/1/2 in this dataset\n",
    "\n",
    "# Optional: choose which treatments to include in PLR (can include multiple)\n",
    "d_cols_plr = d_cols_binary  # you can add more columns here if you want a vector D\n",
    "\n",
    "# Group variable for GATEs (must be mutually exclusive groups for clean interpretation)\n",
    "group_col = \"windex5\"  # wealth quintile (no missing in this dataset)\n",
    "\n",
    "# Sanity checks\n",
    "for c in [y_col, *d_cols_binary, d_col_multilevel, group_col]:\n",
    "    if c not in df_raw.columns:\n",
    "        print(f\"WARNING: column '{c}' not found.\")\n",
    "\n",
    "print(\"Y:\", y_col)\n",
    "print(\"Binary D for IRM:\", d_cols_binary)\n",
    "print(\"Multi-level D for APOS:\", d_col_multilevel)\n",
    "print(\"PLR D columns:\", d_cols_plr)\n",
    "print(\"Group column:\", group_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbcb3c",
   "metadata": {},
   "source": [
    "## 3) Build modeling table: one-hot encode categoricals, impute missing\n",
    "DoubleML expects numeric design matrices. We'll:\n",
    "- Keep `Y` and `D` columns\n",
    "- One-hot encode remaining categorical columns\n",
    "- Median-impute numeric missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only the columns we need + all potential controls\n",
    "# Here we use all other columns as controls by default (high-dimensional W/X).\n",
    "keep_cols = list(dict.fromkeys([y_col] + d_cols_plr + [d_col_multilevel, group_col] + [c for c in df_raw.columns if c not in [y_col] + d_cols_plr + [d_col_multilevel, group_col]]))\n",
    "df = df_raw[keep_cols].copy()\n",
    "\n",
    "# Coerce Y and D columns to numeric where possible\n",
    "df[y_col] = pd.to_numeric(df[y_col], errors=\"coerce\")\n",
    "\n",
    "for c in d_cols_plr:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[d_col_multilevel] = pd.to_numeric(df[d_col_multilevel], errors=\"coerce\")\n",
    "\n",
    "# Build X from all remaining columns excluding y and the specific D columns you include in the model\n",
    "exclude_for_x = set([y_col] + d_cols_plr + [d_col_multilevel])\n",
    "x_cols_raw = [c for c in df.columns if c not in exclude_for_x]\n",
    "\n",
    "# Separate X into categorical vs numeric\n",
    "cat_cols = [c for c in x_cols_raw if df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"category\")]\n",
    "num_cols = [c for c in x_cols_raw if c not in cat_cols]\n",
    "\n",
    "# One-hot encode categoricals (including NaNs as a category)\n",
    "X_cat = pd.get_dummies(df[cat_cols], dummy_na=True) if len(cat_cols) else pd.DataFrame(index=df.index)\n",
    "X_num = df[num_cols].copy()\n",
    "\n",
    "# Coerce numerics\n",
    "for c in X_num.columns:\n",
    "    X_num[c] = pd.to_numeric(X_num[c], errors=\"coerce\")\n",
    "\n",
    "# Combine\n",
    "X = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "# Impute missing values in X\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imp = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "print(\"Raw X cols:\", len(x_cols_raw))\n",
    "print(\"Expanded X cols after one-hot:\", X_imp.shape[1])\n",
    "\n",
    "# Final modeling dataframe for DoubleML: must include y, d, and x columns\n",
    "df_model = pd.concat([df[[y_col] + d_cols_plr].reset_index(drop=True),\n",
    "                      X_imp.reset_index(drop=True)],\n",
    "                     axis=1)\n",
    "\n",
    "# Drop rows with missing y or any missing D in the PLR D columns\n",
    "mask = df_model[y_col].notna()\n",
    "for c in d_cols_plr:\n",
    "    mask &= df_model[c].notna()\n",
    "\n",
    "df_model = df_model.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Final model shape:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c9053",
   "metadata": {},
   "source": [
    "## 4) Define learners (Random Forest) + Optuna hyperparameter spaces\n",
    "We'll tune:\n",
    "- `ml_l` (outcome nuisance) for PLR\n",
    "- `ml_m` (treatment nuisance) for PLR\n",
    "- `ml_g` (outcome nuisance) for IRM/APOS\n",
    "- `ml_m` (propensity nuisance) for IRM/APOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipelines include imputers if you prefer (we already imputed X, but safe)\n",
    "def make_rf_reg(params=None):\n",
    "    params = params or {}\n",
    "    return RandomForestRegressor(\n",
    "        random_state=123,\n",
    "        n_estimators=params.get(\"n_estimators\", 200),\n",
    "        max_depth=params.get(\"max_depth\", None),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "def make_rf_clf(params=None):\n",
    "    params = params or {}\n",
    "    return RandomForestClassifier(\n",
    "        random_state=123,\n",
    "        n_estimators=params.get(\"n_estimators\", 200),\n",
    "        max_depth=params.get(\"max_depth\", None),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "def ml_l_params(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "    }\n",
    "\n",
    "def ml_m_params(trial):\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "    }\n",
    "\n",
    "param_space_plr = {\"ml_l\": ml_l_params, \"ml_m\": ml_m_params}\n",
    "param_space_irm = {\"ml_g\": ml_l_params, \"ml_m\": ml_m_params}  # same shapes; different roles\n",
    "\n",
    "optuna_settings = {\n",
    "    \"n_trials\": 30,\n",
    "    \"show_progress_bar\": True,\n",
    "    \"verbosity\": optuna.logging.WARNING\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448c67f",
   "metadata": {},
   "source": [
    "## 5) PLR: Partial Linear Model (multiple treatments)\n",
    "This estimates a **partial linear causal parameter** (vector if multiple D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build DoubleMLData for PLR\n",
    "x_cols = list(X_imp.columns)  # the one-hot expanded features\n",
    "dml_data_plr = dml.DoubleMLData(df_model, y_col=y_col, d_cols=d_cols_plr, x_cols=x_cols)\n",
    "\n",
    "# Initial learners (will be tuned)\n",
    "ml_l = make_rf_reg()\n",
    "ml_m = make_rf_reg()  # for continuous-ish D; if D is binary you can also try a classifier, but PLR usually uses regression\n",
    "\n",
    "dml_plr = dml.DoubleMLPLR(dml_data_plr, ml_l=ml_l, ml_m=ml_m, n_folds=5)\n",
    "\n",
    "print(dml_plr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216583ad",
   "metadata": {},
   "source": [
    "### 5.1 Tune nuisance models (Optuna)\n",
    "We try DoubleML's `tune_ml_models()` if available; otherwise we fallback to `tune()` (grid/random) as a safe alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ac3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tuning helper ---\n",
    "def tune_with_optuna(dml_obj, param_space, optuna_settings):\n",
    "    if hasattr(dml_obj, \"tune_ml_models\"):\n",
    "        dml_obj.tune_ml_models(ml_param_space=param_space, optuna_settings=optuna_settings)\n",
    "        return \"tune_ml_models\"\n",
    "    elif hasattr(dml_obj, \"tune\"):\n",
    "        # Fallback: randomized search with a reasonable grid approximation\n",
    "        # (If you want Optuna specifically, upgrade DoubleML to a version supporting tune_ml_models.)\n",
    "        par_grids = {\n",
    "            k: {\n",
    "                \"n_estimators\": [100, 200, 300],\n",
    "                \"max_depth\": [3, 5, 8, 12],\n",
    "                \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "            }\n",
    "            for k in param_space.keys()\n",
    "        }\n",
    "        dml_obj.tune(par_grids, search_mode=\"randomized_search\", n_iter_randomized_search=20)\n",
    "        return \"tune(randomized_search)\"\n",
    "    else:\n",
    "        return \"no_tuning_api_found\"\n",
    "\n",
    "used = tune_with_optuna(dml_plr, param_space_plr, optuna_settings)\n",
    "print(\"Tuning method used:\", used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b170bb",
   "metadata": {},
   "source": [
    "### 5.2 Fit + inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d88567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = dml_plr.fit()\n",
    "print(dml_plr.summary)\n",
    "\n",
    "print(\"\\nPointwise CI:\")\n",
    "print(dml_plr.confint())\n",
    "\n",
    "print(\"\\nBootstrap + joint CI:\")\n",
    "_ = dml_plr.bootstrap()\n",
    "print(dml_plr.confint(joint=True))\n",
    "\n",
    "# Sensitivity analysis (choose cf_y, cf_d based on your domain)\n",
    "dml_plr.sensitivity_analysis(cf_y=0.04, cf_d=0.03)\n",
    "print(dml_plr.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651c721",
   "metadata": {},
   "source": [
    "## 5.3 PLR GATEs and CATEs (optional)\n",
    "- **GATE**: requires mutually exclusive groups (e.g., wealth quintile)\n",
    "- **CATE**: provide a low-dimensional basis (e.g., splines on one variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- GATE example (wealth quintile) ---\n",
    "if group_col in df_raw.columns:\n",
    "    # Build groups for the filtered sample used in df_model\n",
    "    # We must align indices: recompute group values for df_model rows (we kept original order)\n",
    "    # We'll rebuild a small aligned series from the same mask used above.\n",
    "    df_tmp = df_raw.loc[mask.values].reset_index(drop=True)  # align with df_model\n",
    "    groups = pd.DataFrame(df_tmp[group_col].astype(str).values, columns=[\"Group\"])\n",
    "    gate_obj = dml_plr.gate(groups=groups)\n",
    "    print(\"PLR-GATE CI:\")\n",
    "    print(gate_obj.confint())\n",
    "else:\n",
    "    print(\"Group col not found; skipping GATE.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7bd414",
   "metadata": {},
   "source": [
    "## 6) IRM: Interactive model (binary treatments only)\n",
    "We fit one IRM per binary treatment and then (optionally) compute GATE/CATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll reuse X_imp columns. Build a base dataframe with y + X only, and then swap in each binary D.\n",
    "base_cols = [y_col] + x_cols\n",
    "df_base = pd.concat([df_model[[y_col]].reset_index(drop=True), df_model[x_cols].reset_index(drop=True)], axis=1)\n",
    "\n",
    "irm_results = {}\n",
    "\n",
    "for d_col in d_cols_binary:\n",
    "    if d_col not in df_raw.columns:\n",
    "        print(f\"Skipping {d_col}: not found.\")\n",
    "        continue\n",
    "\n",
    "    # Build D aligned to df_model sample\n",
    "    d_series = pd.to_numeric(df_raw.loc[mask.values, d_col], errors=\"coerce\").reset_index(drop=True)\n",
    "\n",
    "    # Check binary\n",
    "    vals = sorted(pd.Series(d_series.dropna().unique()).tolist())\n",
    "    if vals != [0.0, 1.0] and vals != [0, 1]:\n",
    "        print(f\"Skipping {d_col}: not binary (unique={vals[:10]}...).\")\n",
    "        continue\n",
    "\n",
    "    df_irm = df_base.copy()\n",
    "    df_irm[d_col] = d_series\n",
    "\n",
    "    # Drop missing D\n",
    "    df_irm = df_irm[df_irm[d_col].notna()].reset_index(drop=True)\n",
    "\n",
    "    dml_data_irm = dml.DoubleMLData(df_irm, y_col=y_col, d_cols=d_col, x_cols=x_cols)\n",
    "\n",
    "    ml_g = make_rf_reg()\n",
    "    ml_m = make_rf_clf()\n",
    "\n",
    "    dml_irm = dml.DoubleMLIRM(dml_data_irm, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "\n",
    "    used = tune_with_optuna(dml_irm, param_space_irm, optuna_settings)\n",
    "    print(f\"[{d_col}] tuning used:\", used)\n",
    "\n",
    "    _ = dml_irm.fit()\n",
    "    irm_results[d_col] = dml_irm\n",
    "\n",
    "    print(f\"\\n=== IRM results for {d_col} ===\")\n",
    "    print(dml_irm.summary)\n",
    "    print(dml_irm.confint())\n",
    "\n",
    "    _ = dml_irm.bootstrap()\n",
    "    print(\"Joint CI:\")\n",
    "    print(dml_irm.confint(joint=True))\n",
    "\n",
    "    dml_irm.sensitivity_analysis(cf_y=0.04, cf_d=0.03)\n",
    "    print(dml_irm.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74f7f4",
   "metadata": {},
   "source": [
    "### 6.1 IRM GATEs (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73205212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if group_col in df_raw.columns:\n",
    "    df_tmp = df_raw.loc[mask.values].reset_index(drop=True)\n",
    "    groups_all = pd.DataFrame(df_tmp[group_col].astype(str).values, columns=[\"Group\"])\n",
    "\n",
    "    for d_col, dml_irm in irm_results.items():\n",
    "        # align group rows to the IRM dataset (may have dropped missing D)\n",
    "        d_series = pd.to_numeric(df_tmp[d_col], errors=\"coerce\")\n",
    "        keep = d_series.notna().values\n",
    "        groups = groups_all.loc[keep].reset_index(drop=True)\n",
    "\n",
    "        gate_obj = dml_irm.gate(groups=groups)\n",
    "        print(f\"\\nIRM-GATE CI for {d_col}:\")\n",
    "        print(gate_obj.confint())\n",
    "else:\n",
    "    print(\"Group col not found; skipping IRM GATE.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c88fe",
   "metadata": {},
   "source": [
    "## 7) APOS: Average Potential Outcomes (discrete multi-level treatment)\n",
    "Useful for treatments like `water_treatment3` with levels 0/1/2. It estimates average potential outcomes by level, then you can form contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c005963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if d_col_multilevel in df_raw.columns:\n",
    "    # Build aligned dataset: y + D + X\n",
    "    d_series = pd.to_numeric(df_raw.loc[mask.values, d_col_multilevel], errors=\"coerce\").reset_index(drop=True)\n",
    "\n",
    "    df_apos = df_base.copy()\n",
    "    df_apos[d_col_multilevel] = d_series\n",
    "    df_apos = df_apos[df_apos[d_col_multilevel].notna()].reset_index(drop=True)\n",
    "\n",
    "    # Identify observed levels\n",
    "    levels = sorted(pd.Series(df_apos[d_col_multilevel].unique()).tolist())\n",
    "    print(\"Observed treatment levels:\", levels)\n",
    "\n",
    "    dml_data_apos = dml.DoubleMLData(df_apos, y_col=y_col, d_cols=d_col_multilevel, x_cols=x_cols)\n",
    "\n",
    "    ml_g = make_rf_reg()\n",
    "    ml_m = make_rf_clf()\n",
    "\n",
    "    dml_apos = dml.DoubleMLAPOS(dml_data_apos, ml_g=ml_g, ml_m=ml_m, treatment_levels=levels, n_folds=5)\n",
    "\n",
    "    used = tune_with_optuna(dml_apos, param_space_irm, optuna_settings)\n",
    "    print(\"APOS tuning used:\", used)\n",
    "\n",
    "    _ = dml_apos.fit()\n",
    "    print(dml_apos.summary)\n",
    "\n",
    "    # Contrast vs reference level 0 if present\n",
    "    ref = 0 if 0 in levels else levels[0]\n",
    "    contrast = dml_apos.causal_contrast(reference_levels=ref)\n",
    "    print(\"\\nCausal contrast summary (ref =\", ref, \"):\")\n",
    "    print(contrast.summary)\n",
    "\n",
    "else:\n",
    "    print(\"Multi-level treatment column not found; skipping APOS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe461fa2",
   "metadata": {},
   "source": [
    "## 8) Diagnostics: Evaluate nuisance learners (optional but recommended for papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    subset = np.logical_not(np.isnan(y_true))\n",
    "    return mean_absolute_error(y_true[subset], y_pred[subset])\n",
    "\n",
    "# Example: evaluate PLR learner for Y nuisance\n",
    "try:\n",
    "    eval_res = dml_plr.evaluate_learners(learners=[\"ml_l\"], metric=mae)\n",
    "    print(eval_res)\n",
    "except Exception as e:\n",
    "    print(\"evaluate_learners not available or failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
